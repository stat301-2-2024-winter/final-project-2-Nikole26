---
title: "Final Report - Exploring Small Business Loan Approval Likelihood"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Nikole Montero Cervantes"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-Nikole26.git](https://github.com/stat301-2-2024-winter/final-project-2-Nikole26.git)

:::

```{r}
#| label: loading-packages-and-data
#| echo: false
library(tidyverse)
library(here)
library(yardstick)
```

## Introduction

The project objective is to develop a predictive model to determine the likelihood of small businesses or start-ups having their loan applications approved by the U.S. Small Business Administration (SBA). This predictive model focuses on classifying loan applications into two categories: "Approved" and "Rejected," utilizing the dependent variable loan status, which indicates if the loan is either "Charged Off" or "Paid in Full".

The motivation behind this project stems from my deep curiosity in understanding the dynamics between financial institutions and emerging enterprises, particularly the challenges startups face in securing loans. As an economics undergraduate interested in applying data science skills and knowledge to real-world business scenarios, this aspect of investigating the relationship between banks and startups caught my attention. By delving into these interactions, this project provides a meaningful way for me to apply my data science expertise in economics.

Predicting loan status is a vital aspect of risk assessment for financial institutions like the SBA. Accurately identifying loans at risk of default can lead to proactive risk management and mitigation strategies. Therefore, developing a robust predictive model can aid in streamlining loan approval processes, optimizing resource allocation, and ultimately reducing the likelihood of loan defaults.

Finally, the dataset used in this project was obtained from the U.S. Small Business Administration (SBA), publicly available on Kaggle, a platform dedicated to data science.

## Data Overview

My original dataset, as summarized in comprises 27 variables and 899,164 observations. The type of variables present in this dataset are character, date, factor, and numeric variables. Besides, missing data is present across several variables, with varying completeness rates. Notably, the variable that shows the date when a loan is declared to be in default exhibits a significant amount of missingness, with only about 18.1% completeness. Other variables such as bank also have missing values, although with higher completeness rates.

### Exploring target variable

When exploring the target variable, the loan status there is an imbalance in the target variable, as seen in @fig-original-target-variable-plot. This is because the loans paid in full (PIF) have a higher presence than the loans being charged off. The missingness of the target variable is minimal. 

```{r}
#| label: fig-original-target-variable-plot
#| fig-cap: Loan Status (target variable) Plot using original dataset
load(here("results/original_mis_status_plot.rda"))
original_mis_status_plot
```

Thus, considering the large amount of observations in my dataset and in order to deal with the imbalance discussed before, I decided to downsample my data by focusing on the target variable, the loan status. This variable has two categories: "Charged Off" (CHGOFF) or "Paid in Full" (PIF). Thus, I consider 15,000 observations for each category, achieving the downsample of my dataset to 30,000 observations. This introduced an artificial balance of the binary target variable, as seen in @fig-downsampled-target-variable-plot, which will address the initial imbalance and missingness of the target variable in the initial data distribution . 

```{r}
#| label: fig-downsampled-target-variable-plot
#| fig-cap: Loan Status (target variable) Plot using downsampled dataset
load(here("results/mis_status_plot.rda"))
mis_status_plot
```

### Downsampled data overview

The downsampled dataset comprises 30,000 observations. It encompasses 27 variables capturing various attributes. Of these variables, 5 are character type, 3 are date type, 3 are factor type, and 16 are numeric type. There is some missingness accross the variables, the variable which the highest amount of missing values is the charge-off date, with about 50.3% of values missing. Nevertheless, the priority, the target variable have a completeness rate of 100%.

## Methods

The following aspects where considering in the methodology to approach this project. 

### Type of prediction problem

This project focus on a classification problem where the goal is to classify loan applications into two categories: "Approved" or "Rejected" based on the loan status variable, the target variable, which indicates whether the loan is "Charged Off" (CHGOFF) or "Paid in Full" (PIF).

### Data Splitting

The data is initially split into training and testing sets using a 75/25 ratio. This means that 75% of the data conforms training data which is used to train the machine learning model. It serves as the basis for the model to learn the patterns and relationships between the input features and the target variable, the loan status. While, 25% of the data that was not used during training will conform the testing data. The best model will then make predictions on this data, and these predictions will be compared against the actual outcomes to assess how well the model generalizes to new, unseen data. 

Stratified splitting is employed in this project to ensure proportional representation of the target variable, which is the loan status, in both the training and testing datasets.

### Models 

##### 1. Naive Bayes:
It is chosen as one of the models due to its simplicity and efficiency, making it suitable for initial exploration of the data. 

##### 2. Binary Logistic Regression:
This model serves as a baseline model for comparison, given its simplicity and interpretability. By estimating the probability of loan approval based on input features, logistic regression can provide valuable insights into the relative importance of predictors and their impact on the loan approval decision.

##### 3. Elastic Net:
This regularized regression method that combines the penalties of both L1 (Lasso) and L2 (Ridge) regularization is useful to deal with high-dimensional data and multicollinearity.

Parameters to be tuned:

* Penalty: The penalty parameter controls the balance between L1 and L2 regularization. It will be tuned over a range of values.

##### 4. K-Nearest Neighbors (KNN):
KNN is included to capture potential non-linear relationships between features and loan approval status. In scenarios where decision boundaries are complex, KNN can provide a flexible and intuitive approach to classification tasks.

Parameters to be tuned:

* Number of neighbors (k): Determines the number of nearest neighbors to consider when making predictions. It will be tuned over a range of values.

##### 5. Boost Trees:
It is an ensemble learning model that combines multiple weak learners (typically decision trees) to create a strong predictive model. It iteratively builds trees, focusing on the mistakes of the previous iterations.

Parameters to be tuned over different values:

* Number of trees: The number of trees to include in the boosting process.

* Learning rate: Controls the contribution of each tree to the final model. It will be tuned over a range of values.

##### 6. Random Forest:
It is another ensemble learning method that constructs a multitude of decision trees at training time and outputs the class that is the mode of the classes of the individual trees.

Parameters to be tuned:

* Number of variables randomly sampled as candidates at each split.

* Minimum node size: The minimum number of data points allowed in a terminal node. It will be tuned over a range of values.

### Recipes

For this project there are two main recipes are considered Recipe 1, consider the baseline recipe, and a more complex one call Recipe 2. The baseline recipe involves steps such as removing irrelevant predictors, handling unknown levels, and one-hot encoding categorical variables. While Recipe 2 includes additional preprocessing steps to handle correlated predictors and normalize the numeric predictors. Both recipes will be used in the models' fittings in order to know which one performs better. 

For the naive Bayes model, two separate recipes are used due to the inability to use a processioning step to one-hot encoding categorical variables. The baseline recipe for this model is simpler, while the enhanced recipe includes additional preprocessing steps such as recoding factors and normalization.

### Metric for Model Comparison

The metric used for model comparison and selection accuracy, as it provides a straightforward measure of each model's overall performance in classifying loan applications correctly. 

### Resampling Technique

The resampling technique used in this project is cross-validation, which is a method for estimating the performance of a predictive model by repeatedly splitting the dataset into training and testing sets. Specifically, the dataset is initially divided into 10 folds. Each fold serves as a holdout set for testing the model, while the remaining data is used for training. This process is repeated 5 times. The data is divided into folds at random within each repetition, resulting in a total of 50 model evaluations

Stratified cross-validation ensures that the distribution of the target variable, the loan status, is preserved across the folds, enhancing the reliability of the evaluation. By repeating the cross-validation process multiple times, any variability due to the random selection of folds is reduced, leading to a more stable and reliable estimate of the model's performance.

## Model Building & Selection

The models analysis and selection will be based on the accuracy of their performances.

```{r}
#| label: fig-results-recipe-1
#| fig-cap: Best Estimated Accuracy per model using Recipe 1
load(here("results/model_results_1.rda"))
model_results_1
```

In @fig-results-recipe-1 it is seen that the model with worst performance is Naive Bayes with a mean 0.610 based on the accuracy as metric. While the best model performing is Random Forest with a mean of 0.995. 

```{r}
#| label: fig-results-recipe-2
#| fig-cap: Best Estimated Accuracy per model using Recipe 1
load(here("results/model_results_2.rda"))
model_results_2
```

In @fig-results-recipe-2 it is seen that the model with worst performance is Naive Bayes with a mean 0.612 based on the accuracy as metric. While the best model performing is Random Forest with a mean of 0.994. 

### Best parameters for each model

The best performance of each model between the two recipes will be selected, and from them the best parameters will be determined for each.

####  Elastic Net

The best penalty parameter is 0.0000000001, indicating that the model heavily penalizes the coefficients of the predictors. Thus, considering that the penalty is already at a very small value, further exploration might not be necessary. Besides, by adjusting the penalty slightly to see if it improves model performance could  sacrifice too much interpretability.

####  K-Nearest Neighbors

The best number of neighbors chosen for the KNN model is 5, which means that the model predicts the outcome based on the 5 nearest neighbors in the feature space. 

####  Boosted Trees

The number of variables randomly sampled at each split that performed best when building the trees is 11. The best minimum amount of observations required in a terminal node of the tree is 16. Finally,The best learning rate is 1.00 indicating full learning from each tree.

####  Random Forest

The best number variables sampled at each split when growing trees in a Random Forest, it's set to 7. While, the minimum number of observations required in a terminal node of the tree that performed best is set to 5.

### Recipes impact on methods' performances

Both recipes show consistent performance across different model types, with small standard errors for each metric. This suggests that the common preprocessing steps (such as removing certain columns and handling unknowns) benefit the stability of the model performance.

Indeed, Recipe 2 includes additional preprocessing steps compared to Recipe 1, such as removing highly correlated predictors and normalizing numeric predictors. However, these additional steps do not seem to significantly improve the overall model performance based on the accuracy metric. As shown in @fig-results-recipe-1 and @fig-results-recipe-2, the mean accuracy values for each model type in Recipe 2 are quite similar to those in Recipe 1. This suggests that the added complexity in preprocessing may not always translate into better predictive performance, at least based on the evaluation metric of accuracy.

Across both recipes, certain model types consistently perform better than others. Specifically, the Boost Tree and Random Forest models tend to have higher accuracy compared to Naive Bayes and KNN models. This is not surprising since the ensemble-based models (Boost Tree and Random Forest) are more effective in capturing the complex relationships in the data compared to simpler models like Naive Bayes and KNN.

## Final Model Analysis

To assess the Random Forest performance the following metrics were considered: confusion matrix, accuracy and the receiver operating characteristic curve.

```{r}
#| label: fig-table-confusion-matrix
#| fig-cap: Confusion Matrix
load(here("results/conf_mat.rda"))
print(conf_mat$table)
```
The values displayed in @fig-table-confusion-matrix are interpreted as it follows:

True Positives: The model correctly predicted 3,742 instances as Charged Off (CHGOFF). These are the instances where the model predicted that the loans would be charged off, and they were indeed charged off.

False Negatives: The model incorrectly predicted 30 instances as Paid In Full (PIF) when they were actually Charged Off (CHGOFF). These are the instances where the model failed to predict that the loans would be charged off when they actually were.

False Positives: The model incorrectly predicted 8 instances as Charged Off (CHGOFF) when they were actually Paid In Full (PIF). These are the instances where the model predicted that the loans would be charged off, but they were actually paid in full.

True Negatives: The model correctly predicted 3,720 instances as "PIF" Paid In Full (PIF). These are the instances where the model predicted that the loans would be paid in full, and they were indeed paid in full.

Overall, based on this confusion matrix, the random forest model demonstrates high accuracy, precision, and recall, indicating strong performance in predicting both Charged Off and Paid In Full classes.

In addition, as @fig-best-model-accuracy shows the accuracy of the final model,the Random Forest model, is 0.9949333. This indicates that the model performed exceptionally well on the test dataset, meaning that it correctly predicted the outcome for approximately 99.49% of the instances in the test dataset.

```{r}
#| label: fig-best-model-accuracy
#| fig-cap: Best Model Accuracy
load(here("results/acc_rf.rda"))
acc_rf
```

```{r}
#| label: fig-roc
#| fig-cap: ROC (Receiver Operating Characteristic Curve) 
load(here("results/roc_curve.rda"))
(roc_curve)
```

Finally, as evidenced in @fig-roc, the area under the ROC curve (AUC) is close to 1. This indicates that the model has excellent discriminatory power, meaning it can effectively distinguish between the two classes, in this case, Charged Off and Paid in Full.

The near-perfect balance between sensitivity and specificity implies that the model effectively balances between correctly identifying positive instances and minimizing false positives, making it highly reliable and suitable when classifying the loan status.

### Does the effort of building a predictive model really pay off?

In the context of loan application approval, accuracy is crucial for making the right decisions about approving or rejecting applications. The random forest model's higher accuracy (0.9949333) compared to the baseline logistic model (0.9879733) indicates a more precise prediction capability. This difference suggests that the random forest model can better differentiate between applicants who should be approved and those who shouldn't, reducing the risk of approving risky loans while also minimizing the rejection of viable applicants.

Moreover, the practical implications of the chosen model's predictions are significant in loan approval decisions. Approving a risky applicant (false positives) can lead to financial losses due to loan defaults, while rejecting a viable applicant (false negatives) can result in missed business opportunities and potential customer dissatisfaction. Therefore, the model's ability to minimize these errors is crucial for the financial health and customer satisfaction of the Small Business Administration (SBA).

Hence, considering the context of loan application approval, the significant improvement in accuracy provided by the random forest model suggests that the effort invested in building it is likely justified.

## Conclusion

While Recipe 2 includes additional preprocessing steps, it does not lead to a substantial improvement in model performance compared to Recipe 1. The choice of model type appears to have a larger impact on performance, with ensemble-based models generally outperforming simpler models.
