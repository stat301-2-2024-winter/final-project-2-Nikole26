---
title: "Final Report"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Nikole Montero Cervantes"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-Nikole26.git](https://github.com/stat301-2-2024-winter/final-project-2-Nikole26.git)

:::

## Introduction

The project objective is to develop a predictive model to determine the likelihood of small businesses or start-ups having their loan applications approved by the U.S. Small Business Administration (SBA). This predictive model focuses on classifying loan applications into two categories: "Approved" and "Rejected," utilizing the dependent variable loan status, which indicates if the loan is either "Charged Off" or "Paid in Full".

The motivation behind this project stems from my deep curiosity in understanding the dynamics between financial institutions and emerging enterprises, particularly the challenges startups face in securing loans. As an economics undergraduate interested in applying data science skills and knowledge to real-world business scenarios, this aspect of investigating the relationship between banks and startups caught my attention. By delving into these interactions, this project provides a meaningful way for me to apply my data science expertise in economics.

Predicting loan status is a vital aspect of risk assessment for financial institutions like the SBA. Accurately identifying loans at risk of default can lead to proactive risk management and mitigation strategies. Therefore, developing a robust predictive model can aid in streamlining loan approval processes, optimizing resource allocation, and ultimately reducing the likelihood of loan defaults.

Finally, the dataset used in this project was obtained from the U.S. Small Business Administration (SBA), publicly available on Kaggle, a platform dedicated to data science.

## Data Overview

## Methods

The following aspects where ocnsidering in the methodoly to apprac this project. 

### Type of prediction problem

This project focus on a classification problem where the goal is to classify loan applications into two categories: "Approved" or "Rejected" based on the loan status variable which indicates whether the loan is "Charged Off" (CHGOFF) or "Paid in Full" (PIF).

### Data Splitting

The data is initially split into training and testing sets using a 75/25 ratio. Stratified splitting is employed to ensure proportional representation of the target variable, which is the loan status, in both the training and testing datasets.

### Models 

1. Naive Bayes:
It is chosen as one of the models due to its simplicity and efficiency, making it suitable for initial exploration of the data. 

2. Binary Logistic Regression:
This model serves as a baseline model for comparison, given its simplicity and interpretability. By estimating the probability of loan approval based on input features, logistic regression can provide valuable insights into the relative importance of predictors and their impact on the loan approval decision.

3. Elastic Net:
This regularized regression method that combines the penalties of both L1 (Lasso) and L2 (Ridge) regularization is useful to deal with high-dimensional data and multicollinearity.

Parameters to be tuned:
- Penalty: The penalty parameter controls the balance between L1 and L2 regularization. It will be tuned over a range of values.

4. K-Nearest Neighbors (KNN):
KNN is included to capture potential non-linear relationships between features and loan approval status. In scenarios where decision boundaries are complex, KNN can provide a flexible and intuitive approach to classification tasks.

Parameters to be tuned:
- Number of neighbors (k): Determines the number of nearest neighbors to consider when making predictions. It will be tuned over a range of values.

5. Boosted Trees:Boosted Trees:
It is an ensemble learning model that combines multiple weak learners (typically decision trees) to create a strong predictive model. It iteratively builds trees, focusing on the mistakes of the previous iterations.

Parameters to be tuned over different values:
- Number of trees: The number of trees to include in the boosting process.
- Learning rate: Controls the contribution of each tree to the final model. It will be tuned over a range of values.

6. Random Forest:
It is another ensemble learning method that constructs a multitude of decision trees at training time and outputs the class that is the mode of the classes of the individual trees.

Parameters to be tuned:
- Number of variables randomly sampled as candidates at each split (mtry).
- Minimum node size (min_n): The minimum number of data points allowed in a terminal node. It will be tuned over a range of values.

### Recipes

For this project there are two main recipes are considered Recipe 1, consider the baseline recipe, and a more complex one call Recipe 2. The baseline recipe involves steps such as removing irrelevant predictors, handling unknown levels, and one-hot encoding categorical variables. While Recipe 2 includes additional preprocessing steps such as handling correlated predictors and normalization of numeric predictors. Both recipes will be used in the models' fittings in order to know which one performs better. 

For the naive Bayes model, two separate recipes are used due to the inability to use a processioning step to one-hot encoding categorical variables. The baseline recipe for this model is simpler, while the enhanced recipe includes additional preprocessing steps such as recoding factors and normalization.

### Resampling Technique

Cross-validation is employed for model evaluation using 10 folds with 5 repeats. This technique helps in estimating the performance of the model on unseen data and reduces the risk of overfitting.

### Metric for Model Comparison

The metric used for model comparison and selection accuracy, as it provides a straightforward measure of each model's overall performance in classifying loan applications correctly. 

## Model Building & Selection


## Final Model Analysis

## Conclusion

